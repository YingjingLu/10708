{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Implementation of EM algorithm [17 pts]\n",
    "\n",
    "3.3.1. Requirements and suggestions\n",
    "\n",
    "3.3.2. Some questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dd\n"
     ]
    }
   ],
   "source": [
    "data = sio.loadmat(\"q3_data.mat\")\n",
    "print(\"dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__globals__', '__header__', '__version__', 'Y', 'X'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(X, Y, rate):\n",
    "    num_sample = X.shape[0]\n",
    "\n",
    "    test_index_set = set(random.sample(range(num_sample), int(num_sample*rate)))\n",
    "    total_index_set = set(range(num_sample))\n",
    "\n",
    "    train_index_set = total_index_set - test_index_set\n",
    "    train_index = list(train_index_set)\n",
    "    test_index = list(test_index_set)\n",
    "\n",
    "    train_x = X[train_index,:]\n",
    "    train_y = Y[train_index, :]\n",
    "\n",
    "    test_x = X[test_index,:]\n",
    "    test_y = Y[test_index,:]\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.71238898]\n",
      " [-4.70767423]\n",
      " [-4.70295949]\n",
      " ..., \n",
      " [ 4.70295949]\n",
      " [ 4.70767423]\n",
      " [ 4.71238898]]\n",
      "(2000, 1)\n",
      "[[ 0.41220109]\n",
      " [ 2.25572172]\n",
      " [ 0.32923398]\n",
      " ..., \n",
      " [-0.11020524]\n",
      " [-0.14431699]\n",
      " [ 0.29909876]]\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data['X']\n",
    "Y = data['Y']\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(Y)\n",
    "print(Y.shape)\n",
    "train_x, train_y, test_x, test_y = train_test_split(X, Y, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1.  Requirements and suggestions\n",
    "\n",
    "In this question, we are going to implement EM algorithm for HME model, following the updating rules derived in part 3.2. Here are some requirements and suggestions for your implementation:\n",
    "\n",
    "### Pad input $\\mathbf{X}$ with $1$\n",
    "\n",
    "To allow bias terms in the model, you should pad $\\mathbf{X}$ with a column with value 1. \n",
    "\n",
    "### IRLS algorithm\n",
    "If you find IRLS algorithm update not stable, i.e. generate singular matrix in the update, you're also allowed to directly use Newton's method, as the IRLS algorithm here is a special case of Newton's method. \n",
    "\n",
    "### Parameter initialization\n",
    "You may explore different ways of parameter initialization, as a suggestion, initialization with uniform distribution Unif(0, 1) for all parameters is fine. \n",
    "\n",
    "### Parameters in the algorithm\n",
    "Set the max iteration as 50 for the main loop. \n",
    "\n",
    "For IRLS algorithm, set max iteration as 5, learning rate $\\lambda$ as 1, using a stopping criterion \n",
    "$$|f^{k+1} - f^{k}| < 1e-6$$\n",
    "where $f$ is the objective function. \n",
    "\n",
    "### The Log Sum Exp trick \n",
    "\n",
    "To reduce numerical error and prevent underflows, you may consider use log-sum-exp trick for the computing of some variables. \n",
    "\n",
    "### Clip some variables\n",
    "In the updating, due to numerical error, some variables, i.e. probabilities, may be very close to 0 or 1, which may make some matrices singular. To avoid this effect, we may need to clip these variables. You may clip them to $[1E{-10}, 1-1E{-10}]$. \n",
    "\n",
    "Hint: you may consider to clip $g_i^{(t)}$, $g_{j|i}^{(t)}$ and $\\mathbb{E}[z^{(t)}_{ij}|\\mathcal{X}]$. \n",
    "\n",
    "### Monitor log-likelihood\n",
    "\n",
    "It is good to monitor the log-likelihood in each iteration, as in EM algorithm, the log-likelihood should keep increasing, which could be a good check of whether the implementation is right. \n",
    "\n",
    "### Program Language\n",
    "\n",
    "You may use either Python or MATLAB in your implementation, as long as you can configure either Python or MATLAB for Jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implemente your algorithm here\n",
    "\n",
    "class IRLS(object):\n",
    "\n",
    "    def __init__(self, threshold, max_iter):\n",
    "        self.threshold = threshold\n",
    "        self.theta = None\n",
    "        self.var = 0\n",
    "        self.delta_param_norm = 0\n",
    "        self.deta_log_like = 0\n",
    "        self.iter = max_iter\n",
    "\n",
    "    def init_param(self, x_dim):\n",
    "        self.theta = np.random.normal(0,1,m)\n",
    "        self.var = 1\n",
    "\n",
    "    def fit(self, X, Y, weights = None):\n",
    "\n",
    "        n,m = X.shape\n",
    "\n",
    "        if weights == None:\n",
    "            w = np.ones(n)\n",
    "            weights = w\n",
    "        else:\n",
    "            w = np.sqrt(weights)\n",
    "\n",
    "        X_w = np.transpose(np.transpose(X) * w)\n",
    "        Y_w = Y*w\n",
    "\n",
    "        if self.theta == None:\n",
    "            self.init_params(m)\n",
    "        theta_tmp = self.theta\n",
    "        var_tmp = self.var \n",
    "        log_likelihood_tmp = self.calc_log_likelihood(X, Y, weights)\n",
    "\n",
    "        self.theta, _, _, _ = LA.lstsq(X, Y)\n",
    "\n",
    "        pred_diff = (Y_w - np.dot(X_w,self.theta))\n",
    "        pred_diff_variance = np.dot(pred_diff,pred_diff)/np.sum(weights)\n",
    "\n",
    "        # update variance\n",
    "        self.var = pred_diff_variance\n",
    "        # recalculate parameters\n",
    "        log_like_after = self.calc_log_likelihood(X,Y,weights)\n",
    "        delta_log_like = ( log_like_after - log_like_before)/n\n",
    "\n",
    "        # undo the update variance if underflow\n",
    "        if delta_log_like < self.stop_learning:\n",
    "            self.theta = theta_recovery\n",
    "            self.var   = var_recovery\n",
    "            delta_log_like = 0\n",
    "\n",
    "        # otherwise save change in parameters and likelihood\n",
    "        theta_delta = self.theta - theta_tmp\n",
    "        self.delta_param_norm = np.sum(np.dot(theta_delta.T,theta_delta))\n",
    "        self.delta_log_like = delta_log_like\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.theta)\n",
    "\n",
    "    def calc_post_log_prob(self, x, y):\n",
    "        if self.thsta == None:\n",
    "            self.init_params(x.shape[1])\n",
    "        # return normal pdf, log pdf\n",
    "        # assume Gaussian\n",
    "        u = y - np.dot(x,self.theta)\n",
    "        log_norm = -1* np.log(np.sqrt(2*np.pi*self.var))\n",
    "        log_main = -u*u/(2*self.var)\n",
    "        log_pdf = log_norm + log_main\n",
    "        prob = np.exp(log_pdf)\n",
    "        return (log_pdf,prob)\n",
    "\n",
    "    def calc_log_likelihood(self, x, y, weights = None):\n",
    "        if weights == None:\n",
    "            weights = np.ones(x,shape[0])\n",
    "        log_pdf, prob = self.calc_post_log_prob(x, y)\n",
    "        loglikelihood = np.sum(weights * log_pdf)\n",
    "\n",
    "        return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2. Some questions\n",
    "\n",
    "Using the provided data ***Question_3.3-data.mat*** as input for your implementation. \n",
    "\n",
    "1. Use M = 2, N = 2 to run your algorithm, report RMSE for the prediction, plot log-likelihood as a function of iteration, plot output $y$ and prediction $\\hat y$ as a function of $x$ in the same plot, and visualize $\\mathbb{E}[z^{(1)}_{ij}|\\mathcal{X}]$, $\\mathbb{E}[z^{(1000)}_{ij}|\\mathcal{X}]$, $i=1:M~~j=1:N$ as heat maps. \n",
    "\n",
    "2. Use M = 5, N = 5 to run your algorithm, report RMSE for the prediction, plot log-likelihood as a function of iteration, plot output $y$ and prediction $\\hat y$ as a function of $x$ in the same plot, and visualize $\\mathbb{E}[z^{(1)}_{ij}|\\mathcal{X}]$, $\\mathbb{E}[z^{(1000)}_{ij}|\\mathcal{X}]$, $i=1:M~~j=1:N$ as heat maps. \n",
    "\n",
    "3. What information could you get form heat maps? i.e. the number of activated expert networks. What's the difference between the two settings in terms of log-likelihood and prediction?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e57de31b2b03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'HME' is not defined"
     ]
    }
   ],
   "source": [
    "hme = HME(train_x, train_y, 2, 2, max_iter = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py3k]",
   "language": "python",
   "name": "Python [py3k]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
